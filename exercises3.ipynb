{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics YTIP2200, part3 - exercises\n",
    "\n",
    "Return .ipynb file to Optima Return box Sun 21th Feb at latest.\n",
    "Write in comment field which excercised you've done fully/partly. You can also comment which parts are done/missing.\n",
    "\n",
    "You can write your solutions in new code cells between existing cells.\n",
    "\n",
    "Don't use for loops etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. Municipalities\n",
    "Read to dataframe a text file containing data from Finnish municipalities (2017-19). Field separator is semicolon and decimal separator comma.\n",
    "\n",
    "http://gpspekka.kapsi.fi/dataanalytics/part3/muncipalities.txt\n",
    "\n",
    "a) Calculate *total area* for each *sub-region* and print 5 sub-regions with largest total area (sub-region name and total area)\n",
    "\n",
    "b) Calculate for each *region* percentage of region's population living in *towns* (as *municipality type*).   \n",
    "Print 5 largest percentages (region name, percentage, population in towns, total population).\n",
    "\n",
    "c) Calculate for each *sub-region* percentage of Swedish speaking people.  \n",
    "Note: can't use average of \"Share of Swedish speakers%\" since municipality populations vary).  \n",
    "Print 5 largest values (sub-region name and precentage of Swedish speaking people).\n",
    " \n",
    " \n",
    "# 1b. access.log\n",
    "Read 3-hour access.log from one server to DataFrame (15 MB zip, 600 000 rows).  \n",
    "\n",
    "http://gpspekka.kapsi.fi/accesslog2.zip\n",
    "\n",
    "Part 3 theory has an axample on access.log  reading.\n",
    "\n",
    "Fields\n",
    "\n",
    "    ip address\n",
    "    ident-id (-)\n",
    "    userid (-)\n",
    "    time\n",
    "    request\n",
    "    status\n",
    "    size\n",
    "    referer\n",
    "    user agent\n",
    "\n",
    "From time field parse hours and minutes and for each minute calculate how many **uniuqe ip addresses** there were.\n",
    "(No need to parse that on timestamps, strings work fine here.)\n",
    "\n",
    "List 10 minutes with most unique ip addresses.\n",
    "\n",
    "Expected result:\n",
    "```\n",
    "12:55    792  \n",
    "12:51    789  \n",
    "12:54    789  \n",
    "12:53    787  \n",
    "12:56    786  \n",
    "12:52    783\n",
    "14:00    773\n",
    "14:01    769\n",
    "13:59    762\n",
    "12:50    760\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1aa) \n",
      "\n",
      "Sub-region\n",
      "Pohjois-Lapin seutukunta    35121.02\n",
      "Itä-Lapin seutukunta        21726.54\n",
      "Tunturi-Lapin seutukunta    21312.16\n",
      "Kehys-Kainuun seutukunta    15434.48\n",
      "Oulunkaaren seutukunta      12240.43\n",
      "Name: Total area, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Don't wrap repr(DataFrame) across additional lines\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "print('1aa) \\n')\n",
    "\n",
    "df = pd.read_csv('http://gpspekka.kapsi.fi/dataanalytics/part3/muncipalities.txt', sep=';', decimal=',')\n",
    "\n",
    "# Five sub-regions with largest Total area\n",
    "print(df.groupby('Sub-region')['Total area'].sum().nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1ab) \n",
      "\n",
      "             Region  Percentage  Population in towns  Total population\n",
      "17      Kymenlaakso   90.171177               156346          173388.0\n",
      "35          Uusimaa   86.882115              1451821         1671024.0\n",
      "21        Pirkanmaa   84.496840               435239          515095.0\n",
      "37  Varsinais-Suomi   82.700352               395789          478582.0\n",
      "5   Etelä-Pohjanmaa   81.707298               155011          189715.0\n"
     ]
    }
   ],
   "source": [
    "print('\\n 1ab) \\n')\n",
    "\n",
    "# Calculate for each region percentage of region's population living in towns (as municipality type).\n",
    "# Print 5 largest percentages (region name, percentage, population in towns, total population).\n",
    "\n",
    "# Ryhmitellään ja summataan populaatio per ryhmä\n",
    "regions = df.groupby(['Region', 'Municipality type']).agg({'Population': 'sum'})\n",
    "\n",
    "# Uudet sarakkeet \n",
    "regions['Percentage'] = regions.groupby(level=0)['Population'].apply(lambda x: 100 * x / x.sum())\n",
    "regions['Total population'] = regions.groupby(level=0)['Population'].apply(lambda x: x/x * x.sum())\n",
    "\n",
    "# reset index, jotta pääsee käsiksi kaikkiin sarakkeisiin\n",
    "flat = regions.reset_index()\n",
    "\n",
    "# Viisi isointa Townia\n",
    "clean = flat[flat['Municipality type']=='Town'].nlargest(5, 'Percentage')\n",
    "\n",
    "# reorder and \"drop\" extra\n",
    "clean = clean[['Region', 'Percentage', 'Population', 'Total population']]\n",
    "\n",
    "# rename columns\n",
    "clean.columns = ['Region', 'Percentage', 'Population in towns', 'Total population']\n",
    "\n",
    "print(clean)\n",
    "\n",
    "\n",
    "# Notes - talteen itselle matkan varrelta\n",
    "# x is group here instead of Series and adding the second new column with same idea will fail (group vs. series https://stackoverflow.com/questions/48479558/clarification-on-lambda-operator-in-pandas-groupby)\n",
    "# population_by_region_mun['Percentage'] = population_by_region_mun.groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))\n",
    "# population_by_region_mun['Total'] = population_by_region_mun.groupby(level=0).apply(lambda x: x/x * float(x.sum()))\n",
    "# print(regions.reset_index().nlargest(5, 'Percentage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1ac) \n",
      "\n",
      "                                    Percentage of swedish speakers\n",
      "Sub-region                                                        \n",
      "Ålands landsbygd                                          0.896688\n",
      "Mariehamns stads ekonomiska region                        0.835000\n",
      "Ålands skärgård                                           0.829686\n",
      "Jakobstadsregionen                                        0.747392\n",
      "Sydösterbottenin seutukunta                               0.663923\n"
     ]
    }
   ],
   "source": [
    "print('\\n 1ac) \\n')\n",
    "\n",
    "# Calculate for each sub-region percentage of Swedish speaking people.\n",
    "# Note: can't use average of \"Share of Swedish speakers%\" since municipality populations vary).\n",
    "# Print 5 largest values (sub-region name and precentage of Swedish speaking people).\n",
    "\n",
    "\n",
    "df['Swedish speakers'] = df['Population'] * df['Share of Swedish speakers%'] / 100.0\n",
    "\n",
    "subr = df.groupby('Sub-region').agg({'Population': 'sum', \n",
    "                             'Swedish speakers': 'sum'})\n",
    "\n",
    "subr['Percentage of swedish speakers'] = subr['Swedish speakers'] / subr['Population']\n",
    "subr = subr.drop(['Swedish speakers', 'Population'], axis = 1)\n",
    "print(subr.nlargest(5, 'Percentage of swedish speakers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1b) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date\n",
       "12:55    792\n",
       "12:51    789\n",
       "12:54    789\n",
       "12:53    787\n",
       "12:56    786\n",
       "12:52    783\n",
       "14:00    773\n",
       "14:01    769\n",
       "13:59    762\n",
       "12:50    760\n",
       "Name: ip, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('1b) \\n')\n",
    "# From time field parse hours and minutes and for each minute calculate how many uniuqe ip addresses there were. \n",
    "# (No need to parse that on timestamps, strings work fine here.)\n",
    "# List 10 minutes with most unique ip addresses.\n",
    "\n",
    "df_log = pd.read_csv('http://gpspekka.kapsi.fi/accesslog2.zip', \n",
    "                sep = ' ', \n",
    "                quotechar = '\"',\n",
    "                usecols = [0,3],  # there are no column labels yet, so need to give implicit integer (column) indices\n",
    "                names = ['ip', 'date'],\n",
    "                converters = {'date' : (lambda x : x[13:18])} # otetaan date-stringistä 13. ja 18. merkin väli\n",
    "                )\n",
    "\n",
    "# df_log.head()\n",
    "\n",
    "df_log.groupby('date')['ip'].nunique().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Crosstab\n",
    "\n",
    "File http://gpspekka.kapsi.fi/dataanalytics/part3/query.csv includes results of imaginary query to company staff.\n",
    "\n",
    "Education-field has values:\n",
    "\n",
    "    1 = basic education \n",
    "    2 = upper secondary education\n",
    "    3 = higher education\n",
    "    4 = third-cycle degree\n",
    "\n",
    "Management and Tasks fields decribe satisfaction to management and work tasks in scale 1-5.\n",
    "\n",
    "Health, Fitness and Timeshare fields have one if person has used these services, otherwise NaN.\n",
    "\n",
    "Aggregate/crosstab/pivot: \n",
    "\n",
    "a) counts with *Family situation* in rows, and *satisfaction to management* in columns (i.e. how many singles have answered on *Management* 1, 2, 3 etc.)\n",
    "\n",
    "b) percentage shares of *satisfaction to management* with *education as text* in rows and share of 1, 2, 3 ... in columns (i.e. how many % of \"basic education\" -rows have answered 1, 2, 3, 4 and 5 etc).\n",
    "\n",
    "c) averages of *satisfaction to management*  and *satisfaction to work tasks* with *age group* (20-29, 30-39, ... , 60-69) in rows\n",
    "\n",
    "d) percentage of men and women on usage of *Health*, *Fitness* and *Timeshare*\n",
    "(what share of men have used health services etc)\n",
    "\n",
    "Model (labels may be different):\n",
    "\n",
    "\n",
    "![model](http://gpspekka.kapsi.fi/dataanalytics/part3/model2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2A) \n",
      "\n",
      "    Number     Sex  Age Family situation  Education  palveluv  Management  Tasks  Health  Fitness  Timeshare\n",
      "0        1  female   38           single        1.0      22.0           3      3     NaN      NaN        NaN\n",
      "1        2  female   29      with family        2.0      10.0           1      3     NaN      NaN        NaN\n",
      "2        3    male   30           single        1.0       7.0           3      3     1.0      NaN        NaN\n",
      "3        4    male   36      with family        1.0      14.0           3      3     1.0      NaN        NaN\n",
      "4        5  female   24           single        2.0       4.0           2      2     1.0      NaN        NaN\n",
      "..     ...     ...  ...              ...        ...       ...         ...    ...     ...      ...        ...\n",
      "77      78    male   22           single        3.0       0.0           4      4     NaN      1.0        1.0\n",
      "78      79    male   33           single        1.0       2.0           1      2     1.0      NaN        NaN\n",
      "79      80    male   27           single        2.0       7.0           3      3     1.0      NaN        1.0\n",
      "80      81    male   35      with family        2.0      16.0           3      3     NaN      NaN        NaN\n",
      "81      82  female   35      with family        3.0      15.0           3      4     1.0      NaN        NaN\n",
      "\n",
      "[82 rows x 11 columns]\n",
      "Management        1  2   3   4  5\n",
      "Family situation                 \n",
      "single            4  7  13   6  1\n",
      "with family       3  9  17  17  5\n",
      "2B) \n",
      "\n",
      "Management     1     2     3     4     5\n",
      "Education                               \n",
      "100.0%     18.5% 11.1% 37.0% 22.2% 11.1%\n",
      "200.0%      6.7% 26.7% 36.7% 26.7%  3.3%\n",
      "300.0%      0.0% 22.7% 36.4% 36.4%  4.5%\n",
      "400.0%      0.0%  0.0% 50.0%  0.0% 50.0%\n",
      "2C) \n",
      "\n",
      "           Management    Tasks\n",
      "AgeGroups                     \n",
      "(20, 30]     3.000000 3.105263\n",
      "(30, 40]     2.914286 3.028571\n",
      "(40, 50]     3.125000 3.375000\n",
      "(50, 60]     3.700000 3.500000\n",
      "(60, 70]     2.000000 5.000000\n",
      "2D) \n",
      "\n",
      "        Health  Fitness  Timeshare\n",
      "Sex                               \n",
      "female   57.1%    23.8%       9.5%\n",
      "male     57.4%    24.6%      11.5%\n"
     ]
    }
   ],
   "source": [
    "print('2A) \\n')\n",
    "data2 = pd.read_csv('http://gpspekka.kapsi.fi/dataanalytics/part3/query.csv', sep=',', decimal=',')\n",
    "print(data2)\n",
    "print(pd.crosstab(data2['Family situation'], data2['Management']))\n",
    "\n",
    "print('2B) \\n')\n",
    "data2b = pd.crosstab(data2['Management'], data2['Education']).apply(lambda x: x / float(x.sum())).T\n",
    "\n",
    "pd.options.display.float_format = '{:.1%}'.format\n",
    "print(data2b)\n",
    "\n",
    "print('2C) \\n')\n",
    "\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "data2c = data2\n",
    "\n",
    "\n",
    "# Lisätään agegroups -sarake\n",
    "data2c['AgeGroups']= pd.cut(data2c['Age'], np.arange(20, 71, 10))\n",
    "c_ans = data2c.groupby(['AgeGroups']).agg({'Management': 'mean','Tasks': 'mean'})\n",
    "print(c_ans)\n",
    "\n",
    "\n",
    "print('2D) \\n')\n",
    "\n",
    "pd.options.display.float_format = '{:.1%}'.format\n",
    "\n",
    "data2d = data2.groupby(['Sex']).agg({'Health': 'count','Fitness': 'count','Timeshare': 'count'})\n",
    "\n",
    "# Seuraaville 4 riville voisi olla tyylikkäämpi ratkaisu...\n",
    "females = len(data2[data2['Sex'] == 'female'])\n",
    "males = len(data2[data2['Sex'] == 'male'])\n",
    "data2d.iloc[0,:] = data2d.iloc[0,:] / females\n",
    "data2d.iloc[1,:] = data2d.iloc[1,:] / males\n",
    "\n",
    "print(data2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Breaks\n",
    "File  http://gpspekka.kapsi.fi/dataanalytics/part3/machine.csv (11 MB) contains data from a machine with 10 second interval. There is timestamp and field *break* that has value 1 when mahcine has been out of use and 0 when device has been running.\n",
    "\n",
    "Pick data on breaks i.e. consecutive 1-rows. Make DataFrame that has for each break:\n",
    "\n",
    "    break number\n",
    "    first timestamp of the break\n",
    "    last timestamp of the break\n",
    "    duration of the break in hours (line count divided by 360)\n",
    "    \n",
    "10 first breaks should look like: \n",
    "\n",
    "\n",
    "```\n",
    "num start \t                        end \t                        duration (h)\t\t\t\n",
    "1 \t2019-03-18T11:26:30.000000Z \t2019-03-18T11:34:30.000000Z \t0.136111\n",
    "2 \t2019-03-18T11:35:30.000000Z \t2019-03-18T11:43:00.000000Z \t0.127778\n",
    "3 \t2019-03-18T14:05:30.000000Z \t2019-03-18T14:16:50.000000Z \t0.191667\n",
    "4 \t2019-03-18T21:49:00.000000Z \t2019-03-18T21:56:00.000000Z \t0.119444\n",
    "5 \t2019-03-18T22:51:20.000000Z \t2019-03-18T22:56:20.000000Z \t0.086111\n",
    "6 \t2019-03-18T22:57:20.000000Z \t2019-03-18T22:59:40.000000Z \t0.041667\n",
    "7 \t2019-03-19T01:47:40.000000Z \t2019-03-19T02:05:30.000000Z \t0.300000\n",
    "8 \t2019-03-19T02:54:40.000000Z \t2019-03-19T02:58:40.000000Z \t0.069444\n",
    "9 \t2019-03-19T03:54:00.000000Z \t2019-03-19T04:00:00.000000Z \t0.102778\n",
    "10\t2019-03-19T04:09:10.000000Z \t2019-03-19T04:20:40.000000Z \t0.194444\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('3) \\n')\n",
    "\n",
    "df_breaks = pd.read_csv('http://gpspekka.kapsi.fi/dataanalytics/part3/machine.csv', \n",
    "                sep = ','\n",
    "#                parse_dates = [0]\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Snow winters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File http://gpspekka.kapsi.fi/dataanalytics/part2/weather.txt has weather data from Jyväskylä 1959-2021. Read file to DataFrame and calculate \"snow sum\" for each winter by adding each day's snow depth (\"snow sum\" is not any meteorological term).\n",
    "\n",
    "Start from winter 1959-60 and end to 2019-20 since 1958-59 and 2020-21 are only partial.\n",
    "\n",
    "Notes:\n",
    "* it is by winter, not by year. Make for example 1st of August as limit.\n",
    "* FMI uses -1 as snow depth when \"there is absolutely no snow at all\". We don't want to reduce snow sum with that so replace -1 -> 0\n",
    "* for some reason there are few missing snow depths on the data. If there has been snow that gives wrong sum. It is safely to assume that snow depth has been approximately same as previous day so fill NaNs with previous valid value\n",
    "\n",
    "Then produce DataFrame that has winter as index (in form \"1959-1960\") and columns\n",
    "* snow sum\n",
    "* snow sum rank among winters so that largest = 1\n",
    "* count of days that snow depth has been over zero\n",
    "* largest snow depth of the winter\n",
    "\n",
    "\n",
    "10 first and 10 last should look like:\n",
    "```\n",
    "           Snow sum  rank  count  max\n",
    "Winter                               \n",
    "1959-1960      5593    18    169   65\n",
    "1960-1961      5082    28    162   60\n",
    "1961-1962      6644    12    156   78\n",
    "1962-1963      3984    43    158   50\n",
    "1963-1964      4512    38    166   49\n",
    "1964-1965      5465    20    159   62\n",
    "1965-1966      9053     3    185   86\n",
    "1966-1967      5818    16    164   74\n",
    "1967-1968      5150    25    161   64\n",
    "1968-1969      7863     5    195   90\n",
    "           Snow sum  rank  count  max\n",
    "Winter                               \n",
    "2010-2011      6670    11    174   71\n",
    "2011-2012      4582    37    148   59\n",
    "2012-2013      5231    22    153   59\n",
    "2013-2014       644    61     95   17\n",
    "2014-2015      3314    46    134   47\n",
    "2015-2016      2002    55    110   40\n",
    "2016-2017      2500    54    153   38\n",
    "2017-2018      6882     8    161   81\n",
    "2018-2019      4030    42    150   54\n",
    "2019-2020      1432    59    112   30\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) \n",
      "\n",
      "           Snow sum  rank  count  max\n",
      "Winter                               \n",
      "1959-1960      5593    18    169   65\n",
      "1960-1961      5082    28    162   60\n",
      "1961-1962      6644    12    156   78\n",
      "1962-1963      3984    43    158   50\n",
      "1963-1964      4512    38    166   49\n",
      "1964-1965      5465    20    159   62\n",
      "1965-1966      9053     3    185   86\n",
      "1966-1967      5818    16    164   74\n",
      "1967-1968      5150    25    161   64\n",
      "1968-1969      7863     5    195   90\n",
      "           Snow sum  rank  count  max\n",
      "Winter                               \n",
      "2010-2011      6670    11    174   71\n",
      "2011-2012      4582    37    148   59\n",
      "2012-2013      5231    22    153   59\n",
      "2013-2014       644    61     95   17\n",
      "2014-2015      3314    46    134   47\n",
      "2015-2016      2002    55    110   40\n",
      "2016-2017      2500    54    153   38\n",
      "2017-2018      6882     8    161   81\n",
      "2018-2019      4030    42    150   54\n",
      "2019-2020      1432    59    112   30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Snow depth (cm)</th>\n",
       "      <th>Winters</th>\n",
       "      <th>Winter days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1959</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1959-1960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1959</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1959-1960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1959</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1959-1960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1959</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1959-1960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1959</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1959-1960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22488</th>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489</th>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22490</th>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22491</th>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22492</th>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22281 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year  Month  Day  Snow depth (cm)    Winters  Winter days\n",
       "212    1959      8    1             0.0%  1959-1960            0\n",
       "213    1959      8    2             0.0%  1959-1960            0\n",
       "214    1959      8    3             0.0%  1959-1960            0\n",
       "215    1959      8    4             0.0%  1959-1960            0\n",
       "216    1959      8    5             0.0%  1959-1960            0\n",
       "...     ...    ...  ...              ...        ...          ...\n",
       "22488  2020      7   27             0.0%  2019-2020            0\n",
       "22489  2020      7   28             0.0%  2019-2020            0\n",
       "22490  2020      7   29             0.0%  2019-2020            0\n",
       "22491  2020      7   30             0.0%  2019-2020            0\n",
       "22492  2020      7   31             0.0%  2019-2020            0\n",
       "\n",
       "[22281 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('4) \\n')\n",
    "\n",
    "df_weather = pd.read_csv('http://gpspekka.kapsi.fi/dataanalytics/part2/weather.txt', \n",
    "                sep = ',',\n",
    "                decimal='.',\n",
    "                usecols = [0, 1, 2, 6],  \n",
    "                # parse_dates = [['Year', 'Month', 'Day']]\n",
    "                )\n",
    "# FMI uses -1 as snow depth when \"there is absolutely no snow at all\". \n",
    "# We don't want to reduce snow sum with that so replace -1 -> 0\n",
    "df_weather['Snow depth (cm)'].replace(-1, 0, inplace = True)\n",
    "\n",
    "\n",
    "# Start from winter 1959-60 and end to 2019-20, since 1958-59 and 2020-21 are only partial.\n",
    "# Clean \n",
    "is_year = df_weather['Year'] == 1959\n",
    "is_month = df_weather['Month'] < 8\n",
    "df_weather = df_weather[~(is_year & is_month)]\n",
    "\n",
    "# Clean\n",
    "is_year = df_weather['Year'] == 2020\n",
    "is_month = df_weather['Month'] > 7\n",
    "df_weather = df_weather[~(is_year & is_month)]\n",
    "df_weather = df_weather[df_weather['Year'] != 2021]\n",
    "\n",
    "# Replace few missing snow depths in the data by filling NaNs with previous valid value\n",
    "df_weather = df_weather.fillna(method='ffill')\n",
    "\n",
    "# Labels for seasons\n",
    "start_years = list(range(1959,2020))\n",
    "seasons = [str(x) + '-' + str(x+1) for x in start_years]\n",
    "\n",
    "# Bins. Rangen alalaita eka index, korotus 366, paitsi karkausvuonna yksi enemmän \n",
    "# Rangen ylälaita kaivettu kokeilemalla ja laittamalla yksi yli viimeisen bin:n...)\n",
    "bins = np.arange(210, 22537, 366)\n",
    "for i in range(0, len(bins)):\n",
    "    if i % 4 == 0:\n",
    "        bins[i] += 1\n",
    "\n",
    "# print(bins)\n",
    "\n",
    "# Lisätään sarake talvikausille\n",
    "df_weather['Winters']= pd.cut(df_weather.index, bins, labels = seasons)\n",
    "#print(df_weather)\n",
    "\n",
    "# Jos lunta, laitetaan päivälle arvoksi 1, muuten 0\n",
    "df_weather['Winter days']= np.where(df_weather['Snow depth (cm)'] > 0, 1, 0)\n",
    "\n",
    "\n",
    "df_agg = df_weather.groupby('Winters').agg({'Snow depth (cm)': ['sum','max'],'Winter days': 'sum'}).reset_index()\n",
    "df_agg['rank'] = df_agg['Snow depth (cm)']['sum'].rank(ascending=False).astype(int)\n",
    "\n",
    "\n",
    "# set the columns to the top level, to flatten hierarchical index in columns\n",
    "df_agg.columns = df_agg.columns.get_level_values(0)\n",
    "\n",
    "# rename columns\n",
    "df_agg.columns = ['Winter', 'Snow sum', 'max', 'count', 'rank']\n",
    "\n",
    "# cleaning floats to int\n",
    "df_agg['Snow sum'] = df_agg['Snow sum'].astype(int)\n",
    "df_agg['max'] = df_agg['max'].astype(int)\n",
    "\n",
    "# setting Winter as index\n",
    "df_agg = df_agg.set_index('Winter')\n",
    "\n",
    "# reorder columns\n",
    "df_agg = df_agg[['Snow sum', 'rank', 'count', 'max']]\n",
    "\n",
    "print(df_agg.head(10))\n",
    "print(df_agg.tail(10))\n",
    "\n",
    "\n",
    "\n",
    "# Itselle talteen\n",
    "\n",
    "# Start from winter 1959-60 and end to 2019-20, since 1958-59 and 2020-21 are only partial.\n",
    "# df_weather = df_weather[(df_weather['Year_Month_Day'] > '1959-07-31') & (df_weather['Year_Month_Day'] < '2020-08-01')]\n",
    "\n",
    "# korvaaminen onnistuu näinkin, mutta hoidettu yläpuolella matskussa esitellyllä replacellä\n",
    "# df_weather.loc[df_weather['Snow depth (cm)'] == -1, 'Snow depth (cm)'] = 0 \n",
    "# df_weather.tail(40)\n",
    "\n",
    "# NaNien tarkistamiseen - onko niitä, missä? onnistuiko täyttö?\n",
    "# print(df_weather[df_weather.isna().any(axis=1)])\n",
    "# näin voi printata tietyssä indeksissä olevan rivin \n",
    "# print(df_weather.loc[[17350]])\n",
    "\n",
    "# Summed by month and reset index\n",
    "# df_monthly_sums = df_weather.groupby(['Year', 'Month']).agg('Snow depth (cm)').sum().reset_index()\n",
    "\n",
    "# Count months for range in binning\n",
    "# months = df_monthly_sums['Month'].count()\n",
    "\n",
    "# Add seasons and bin them by month\n",
    "# df_monthly_sums['Winters'] = pd.cut(df_monthly_sums.index, np.arange(-1, months, 12), labels = seasons)\n",
    "\n",
    "# Hierarkisen indexin vuoksi ajatus kopioida ja poistaa, mutta meni rumaksi, hankalaksi ja olisi pitänyt toistaa\n",
    "# df_agg['count'] = df_agg['Winter days']['sum']\n",
    "# df_agg.drop(['Winter days'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. League Table \n",
    "File http://gpspekka.kapsi.fi/dataanalytics/part3/epl1718.txt has English Premier league results from season 2017-18.\n",
    "\n",
    "Read it to DataFrame and generate league table, which has team name as index and columns:\n",
    "* games played\n",
    "* wins\n",
    "* draws\n",
    "* defeats\n",
    "* goals for - goals against\n",
    "* points\n",
    "\n",
    "(win gives 3 points and draw gives 1 point)\n",
    "\n",
    "Sort it with points (most points win). If points are equal, then sorted by\n",
    "* goal difference (goals for - goals against)\n",
    "* goals for\n",
    "\n",
    "Fields needed:\n",
    " * HomeTeam\n",
    " * AwayTeam\n",
    " * FTHG (fulltime home goals)\n",
    " * FTAG (fulltime away goals)\n",
    "\n",
    "Expected result:\n",
    "```\n",
    "                games  wins  draws  defeats   goals  points\n",
    "Man City           38    32      4        2  106-27     100\n",
    "Man United         38    25      6        7   68-28      81\n",
    "Tottenham          38    23      8        7   74-36      77\n",
    "Liverpool          38    21     12        5   84-38      75\n",
    "Chelsea            38    21      7       10   62-38      70\n",
    "Arsenal            38    19      6       13   74-51      63\n",
    "Burnley            38    14     12       12   36-39      54\n",
    "Everton            38    13     10       15   44-58      49\n",
    "Leicester          38    12     11       15   56-60      47\n",
    "Newcastle          38    12      8       18   39-47      44\n",
    "Crystal Palace     38    11     11       16   45-55      44\n",
    "Bournemouth        38    11     11       16   45-61      44\n",
    "West Ham           38    10     12       16   48-68      42\n",
    "Watford            38    11      8       19   44-64      41\n",
    "Brighton           38     9     13       16   34-54      40\n",
    "Huddersfield       38     9     10       19   28-58      37\n",
    "Southampton        38     7     15       16   37-56      36\n",
    "Swansea            38     8      9       21   28-56      33\n",
    "Stoke              38     7     12       19   35-68      33\n",
    "West Brom          38     6     13       19   31-56      31\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>games</th>\n",
       "      <th>wins</th>\n",
       "      <th>draws</th>\n",
       "      <th>defeats</th>\n",
       "      <th>goals</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Team</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Man City</th>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>106-27</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man United</th>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>68-28</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tottenham</th>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>74-36</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Liverpool</th>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>84-38</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chelsea</th>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>62-38</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arsenal</th>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>74-51</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burnley</th>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>36-39</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Everton</th>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>44-58</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leicester</th>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>56-60</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crystal Palace</th>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>45-55</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bournemouth</th>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>45-61</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Newcastle</th>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>39-47</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Ham</th>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>48-68</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Watford</th>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>44-64</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brighton</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>34-54</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Huddersfield</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>28-58</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southampton</th>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>37-56</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stoke</th>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>35-68</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swansea</th>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>28-56</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Brom</th>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>31-56</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                games  wins  draws  defeats   goals  points\n",
       "Team                                                       \n",
       "Man City           38    32      4        2  106-27     100\n",
       "Man United         38    25      6        7   68-28      81\n",
       "Tottenham          38    23      8        7   74-36      77\n",
       "Liverpool          38    21     12        5   84-38      75\n",
       "Chelsea            38    21      7       10   62-38      70\n",
       "Arsenal            38    19      6       13   74-51      63\n",
       "Burnley            38    14     12       12   36-39      54\n",
       "Everton            38    13     10       15   44-58      49\n",
       "Leicester          38    12     11       15   56-60      47\n",
       "Crystal Palace     38    11     11       16   45-55      44\n",
       "Bournemouth        38    11     11       16   45-61      44\n",
       "Newcastle          38    12      8       18   39-47      44\n",
       "West Ham           38    10     12       16   48-68      42\n",
       "Watford            38    11      8       19   44-64      41\n",
       "Brighton           38     9     13       16   34-54      40\n",
       "Huddersfield       38     9     10       19   28-58      37\n",
       "Southampton        38     7     15       16   37-56      36\n",
       "Stoke              38     7     12       19   35-68      33\n",
       "Swansea            38     8      9       21   28-56      33\n",
       "West Brom          38     6     13       19   31-56      31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run into solution linked below with functions and cool grouping, while researching for this task and it was so nicely done, \n",
    "# I could not go with any other solution\n",
    "# edited from source https://alysivji.github.io/pl-analysis-part1-finding-longest-consecutive-position-streak.html\n",
    "\n",
    "\n",
    "df_league = pd.read_csv('http://gpspekka.kapsi.fi/dataanalytics/part3/epl1718.txt', \n",
    "                sep = ',',\n",
    "                usecols = ['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'Referee'],\n",
    "                )\n",
    "\n",
    "\n",
    "df_league.head()\n",
    "\n",
    "df_league['H'] = df_league['HomeTeam']\n",
    "df_league['A'] = df_league['AwayTeam']\n",
    "\n",
    "cols_to_keep = ['Date', 'HomeTeam', 'AwayTeam', 'FTHG',\n",
    "                'FTAG', 'Referee']\n",
    "\n",
    "team_results = pd.melt(\n",
    "    df_league, \n",
    "    id_vars=cols_to_keep, \n",
    "    value_vars=['H', 'A'],\n",
    "    var_name='Home/Away',\n",
    "    value_name='Team')\n",
    "\n",
    "\n",
    "team_results['Opponent'] = np.where(team_results['Team'] == team_results['HomeTeam'],\n",
    "                                    team_results['AwayTeam'],\n",
    "                                    team_results['HomeTeam'])\n",
    "\n",
    "points_map = {\n",
    "    'W': 3,\n",
    "    'D': 1,\n",
    "    'L': 0\n",
    "}\n",
    "\n",
    "def get_result(score, score_opp):\n",
    "    if score == score_opp:\n",
    "        return 'D'\n",
    "    elif score > score_opp:\n",
    "        return 'W'\n",
    "    else:\n",
    "        return 'L'\n",
    "\n",
    "\n",
    "# full time goals\n",
    "team_results['Goals'] = np.where(team_results['Team'] == team_results['HomeTeam'],\n",
    "                                 team_results['FTHG'],\n",
    "                                 team_results['FTAG'])\n",
    "team_results['Goals_Opp'] = np.where(team_results['Team'] != team_results['HomeTeam'],\n",
    "                                     team_results['FTHG'],\n",
    "                                     team_results['FTAG'])\n",
    "team_results['Result'] = np.vectorize(get_result)(team_results['Goals'], team_results['Goals_Opp'])\n",
    "team_results['points'] = team_results['Result'].map(points_map)\n",
    "\n",
    "  \n",
    "# Drop unnecessary columns and sort by date\n",
    "cols_to_drop = ['HomeTeam', 'AwayTeam', 'FTHG', 'FTAG']\n",
    "team_results = (team_results\n",
    "                    .drop(cols_to_drop, axis=1)\n",
    "                    .sort_values(by=['Date', 'Referee']))\n",
    "    \n",
    "# Testing the standings function \n",
    "(team_results\n",
    "     .groupby('Team')\n",
    "     .sum()['points']\n",
    "     .sort_values(ascending=False))\n",
    "\n",
    "def standings(frame, result_col, goals_col, goals_opp_col, points_col):\n",
    "    \"\"\"This function takes in a DataFrame and strings identifying fields\n",
    "    to calculate the league table.\n",
    "    \n",
    "    Making it generalized will allow us to calculate league tables for\n",
    "    First Half Goals only. Second Half Goals only.\n",
    "    \"\"\"\n",
    "    record = {}\n",
    "    \n",
    "    record['games'] = np.size(frame[result_col])\n",
    "    record['wins'] = np.sum(frame[result_col] == 'W')\n",
    "    record['draws'] = np.sum(frame[result_col] == 'D')\n",
    "    record['defeats'] = np.sum(frame[result_col] == 'L')\n",
    "    record['goals'] = str(np.sum(frame[goals_col])) + '-' + str(np.sum(frame[goals_opp_col]))\n",
    "    record['points'] = np.sum(frame[points_col])\n",
    "    \n",
    "    \n",
    "    return pd.Series(record,\n",
    "                     index=['games', 'wins', 'draws', 'defeats', 'goals', \"points\"])\n",
    "\n",
    "# Get League Table\n",
    "results_byteam = team_results.groupby(['Team'])\n",
    "\n",
    "(results_byteam \n",
    "     .apply(standings,\n",
    "            result_col='Result',\n",
    "            goals_col='Goals',\n",
    "            goals_opp_col='Goals_Opp',\n",
    "            points_col='points')\n",
    "     .sort_values('points', ascending=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
